{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582f9586-ca27-4094-bb67-3038aa7a6644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ravee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ravee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ravee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ravee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ravee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvader_lexicon\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification, AutoTokenizer\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#The details of post and the API endpoint\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import wordcloud \n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import emoji\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import io\n",
    "import unicodedata\n",
    "import re\n",
    "import string\n",
    "from numpy import linalg\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('vader_lexicon')\n",
    "import requests\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "#The details of post and the API endpoint\n",
    "ACCESS_TOKEN = 'EABCH0JqmrmABO8T8fMAq2ZBWZAfRCbv7Umko6GmV2J8X2sLzuaRfSS31RU16ZBnzAxdEIpioYdFhTm4kgvpjpPU3GrZBfN5DFImvFhEV9P68Xg4oM8w1APUfGJQndrZB7c4hPFvGGxXHDW657Ezq5qZAd840VpAZBBoaqmh6ADM5tDuBG6Hz6NZBEEnDC7OBm58ZAG7l1fUeFfyg1UJf9I74YlJiMHqZCEZCha7KUgF8uAOU4xSjx4Cs25KZAsMRV0ZBB5AZDZD'\n",
    "POST_ID = '18KCp9sP6E'\n",
    "url = 'https://graph.facebook.com/v22.0/18KCp9sP6E/comments?access_token=EABCH0JqmrmABO8T8fMAq2ZBWZAfRCbv7Umko6GmV2J8X2sLzuaRfSS31RU16ZBnzAxdEIpioYdFhTm4kgvpjpPU3GrZBfN5DFImvFhEV9P68Xg4oM8w1APUfGJQndrZB7c4hPFvGGxXHDW657Ezq5qZAd840VpAZBBoaqmh6ADM5tDuBG6Hz6NZBEEnDC7OBm58ZAG7l1fUeFfyg1UJf9I74YlJiMHqZCEZCha7KUgF8uAOU4xSjx4Cs25KZAsMRV0ZBB5AZDZD'\n",
    "\n",
    "#getting the comments as row http responses and stored in a response object\n",
    "response = requests.get(url)\n",
    "\n",
    "#Error handling in a user friendly way especially when fetching comments\n",
    "if response.status_code == 200:\n",
    "    print('Server error, Retrying...')\n",
    "if response.status_code == 400:\n",
    "    print('Issue with the request like invalid or expires access token')\n",
    "if response.status_code == 401:\n",
    "    print('Invalid access token. Please generate a new one')\n",
    "if response.status_code == 403:\n",
    "    print('Permission denied. Check your app permissions')\n",
    "if response.status_code == 404:\n",
    "    print('Post not found. Please verify the post ID')\n",
    "else:\n",
    "    print('Error fetching comments', response.json())\n",
    "\n",
    "#converting from raw object to a jason object\n",
    "comments_data = response.json()\n",
    "comments = []\n",
    "#iterate through json object and add comments to a comments list\n",
    "for comment in comments_data.get('data',[]):\n",
    "    comments.append(comment['message'])\n",
    "\n",
    "#Since fb provide lismited API privilages, not able to fetch the all the comments at once\n",
    "while 'next' in comments_data.get('paging', {}):\n",
    "    next_page = comments_data['paging']['next']\n",
    "    response = requests.get(next_page)\n",
    "    comments_data =response.json()\n",
    "    for comment in comments_data.get('data',[]):\n",
    "        comments.append(comment['message'])\n",
    "\n",
    "#Analyze Sarcasm using deep learning\n",
    "#Load a pre-trained sarcasm detection BERT based model\n",
    "model_name = \"mrm8488/bert-mini-finetuned-sarcasm-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "\n",
    "#Ensure model runs on GPU if available\n",
    "device = torch.device('cuda' if torch.cuda_is_available() else cpu) \n",
    "model.to(device) \n",
    "\n",
    "#now, comments list has all the comments.\n",
    "for comment in comments:\n",
    "    #Define a function to detect sarcasm\n",
    "    def detect_sarcasm(text):\n",
    "        inputs = tokenizer(text, return_tensors = 'pt', truncation = True, padding = True)\n",
    "        inputs = {key:val.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim =1).item()\n",
    "\n",
    "    return 'Sarcastic' if prediction = 1 else 'Not Sarcastic'\n",
    "        \n",
    "    \n",
    "    #extracct emojis and convert it to a text description and combine all the elements of the sentence\n",
    "    comment_with_emojis =emoji.demojize(comment, delimeter =('','')) \n",
    "    \n",
    "    #extract hashtags with camel case \n",
    "    hashtags_word_list = re.findall(r'#([A-Z]?[a-z]+|[A-Z]+(?![a-z]))+',comment_with_emojis)\n",
    "    full_sentence = comment_with_emojis + ''.join(hashtags_word_list)\n",
    "    sentiment_score=sid.polarity_score(full_sentence)\n",
    "    \n",
    "    #calculate sentinmenst for each category and updating te dictionary\n",
    "    sentiment_results = {'Positive':0, 'Negative':0, 'Neutral':0}\n",
    "    if sentiment_score['compound'] > 0.05:\n",
    "        sentiment_results['Positive'] += 1\n",
    "    if sentiment_score['compound']< -0.05:\n",
    "        sentiment_results['Negative'] += 1\n",
    "    else:\n",
    "        sentiment_results['Neutral'] += 1\n",
    "    print(comment_with_emojis)\n",
    "    print(sorted(sentiment_score))\n",
    "    print('/n')\n",
    "\n",
    "#Taking percentages \n",
    "total_positive_comments = sentiment_results['Positive'] \n",
    "total_negative_comments = sentiment_results['Negative']\n",
    "total_neutral_comments = sentiment_results['Neutral']\n",
    "total_comments = total_positive_comments + total_negative_comments + total_neutral_commments\n",
    "positive_percentage =(total_positive_comments/total_comments)*100\n",
    "negative_percentage = (total_negative_comments/total_comments)*100\n",
    "neutral_percentage = (total_neutral_comments/total_comments)*100\n",
    "\n",
    "print('Percentage of positive commnets: ', positive_percentage,'%')\n",
    "print('Percentage of negative comments: ', negative_percentage, '%')\n",
    "print('Percentage of neutral comments: ', neutral_percentage, '%')\n",
    "\n",
    "#data visualization(bar graph)\n",
    "sentiments = ['Positive','Negative','Neutral']\n",
    "Counts = [total_positive_comments, total_negative_comments,total_neutral_comments]\n",
    "sns.barplot(x=sentiments, y=counts, palette =['green', 'gray', 'red'])\n",
    "\n",
    "#labels\n",
    "plt.xlabel('Sentiments')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.show()\n",
    "\n",
    "#Analyze Sarcasm using deep learning\n",
    "#Load a pre-trained sarcasm detection BERT based model\n",
    "model_name = \"mrm8488/bert-mini-finetuned-sarcasm-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "\n",
    "#Ensure model runs on GPU if available\n",
    "device = torch.device('cuda' if torch.cuda_is_available() else cpu) \n",
    "model.to(device) \n",
    "\n",
    "#Define a function to detect sarcasm\n",
    "def detect_sarcasm(text):\n",
    "    inputs = tokenizer(text, return_tensors = 'pt', truncation = True, padding = True)\n",
    "    inputs = {key:val.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim =1).item()\n",
    "\n",
    "    return 'Sarcastic' if prediction = 1 else 'Not Sarcastic'\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b656808-3ad6-4320-b872-b6153f5c9e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c27525-278b-43c6-b2da-eb7297c7ece5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4363a-d583-43a2-bcfc-e6e5ce677c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb5a15-fec5-4840-8c0f-b772a69c361a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09956ef3-81bf-4ea0-99a0-f39463bf626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
